---
title: CubeNet - Equivariance to 3D Rotation and Translation
description: Paper Review
header: CubeNet&#58; Equivariance to 3D Rotation and Translation
duration: 2 minute read
---

&nbsp;

Typical convolutional networks are translational (but not rotational) equivariant. Nevertheless rotational equivariance is still important since objects are invariant to object poses. CubeNet proposes a 3D Group Convolutional Neural Network with linear equivariance to translations and _right angle_ rotations in three dimensions.

### Equivariance

Section 1 first describes what is meant by equivariance. Consider a transformation $g \in G$ where $G$ is a [group] of transformations, and a function $\Phi : X \to Y $  mapping inputs $x \in X$ to outputs $y \in Y$. The function $\Phi$ is equivariant if:

$$\Phi(T_g x) = T_g' \Phi(x)$$

i.e. transforming the input $x$ by $g$ (giving $T_g$), then passing it through the function $\Phi$ gives the same result as passing it through the function $\Phi$ first then transforming the representation. Note that $T_g$ and $T_g'$ need not be the same, but we focus on the case where $T_g$ and $T_g'$ are linear functions of _g_. In this case, $\Phi$ is _linearly equivariant_. 

A special case of equivariance is invariance, where $T_g'$ is identity for all $g$, i.e.:

$$\Phi(\Tau_g x) = \Phi(x)$$

In this case, even when the input is transformed, the output feature map remains the same. As information about the transform $g$ is lost, general equivariance may be more useful in some cases than invariance.

### Group Convolution

Section 2 then discusses what is group convolution \[[1]\]. We first consider the case of the typical convolution, shown in equation (4) in the paper. 

$$[F * W]_g = \sum_{x \in \mathbb{Z}^3} [gW]_x [F]_x = \sum_{x \in \mathbb{Z}^3} [W]_{g^{-1}x} [F]_x$$

From the first equality, we see that this is equivalent to "translate filter and compute inner product". This is also illustrated in Fig. 1 left. The second equality shows that rather than translating the filter $W$, we can just simply retrieve the filter weight at position $g^{-1}x$. 

In a _Group Convolution_, the translation is generalized to a _transformation_ $g \in G$, giving us equation (6).

$$[F * W]_g = \sum_{x \in G} [gW]_x [F]_x = \sum_{x \in G} [W]_{g^{-1}x} [F]_x$$

Notice that output feature map is no longer a function in a plane (or translation feature group), but instead is a function of group $G$. This will have implications on how we do convolutions in layers after the first one.

### My Comments

I actually do not quite understand some details in the paper, particularly in equation (11):

$$[F*W]_{tr} = \sum_{\tau \in \mathbb{Z}^3} \sum_{\rho \in V} [tr W]_{\tau \rho} F_{\tau \rho} = \sum_{\tau \in \mathbb{Z}^3} \sum_{\rho \in V} [t[rW]_\rho]_{\tau} F_{\tau \rho} $$

This should mean that the output has size $$H \times W \times D \times \text{num_rotations}$$. But Fig. 4 seems to just imply that it's just $$H \times W \times D$$. Is this because of the global average pooling (briefly stated in the ModelNet10 section)? Is the average pooling only in the rotational dimension? This is not clear to me. Also, if that is true, doesn't the rotational equivariance become invariance?



\[[paper]\]

[group]: https://en.wikipedia.org/wiki/Group_(mathematics)
[1]: https://arxiv.org/abs/1602.07576
[paper]:https://arxiv.org/abs/1804.04458

&nbsp;

