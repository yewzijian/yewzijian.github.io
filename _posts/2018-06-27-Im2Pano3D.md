---
title: Im2Pano3D&#58; Extrapolating 360deg Structure and Semantics Beyond the Field of View
description: Paper Summary
header: Im2Pano3D&#58; Extrapolating 360deg Structure and Semantics Beyond the Field of View
duration: 5 minutes read
categories: blog
---

(CVPR 2018)

## Objective

3D Structure and Semantics completion and prediction.

## Network

![Network](/blog_images/2018-06-27_14-49-03.png)



### Overview

* Represented using 360deg (spherical projected) image
* Generator:
  * <u>Inputs</u>: plane parameters at each pixel (distance $$p$$ and normal $$n$$), and optionally RGB+semantic labels $$s$$. For pixels in the field of view
  * <u>Output</u>: $$p, n, s$$ for all pixels.
* PNLayer:
  * Generates the 3D points given the plane parameters $$p, n$$
  * This can be compared with the groundtruth points to compute a L1 loss.
  * This is beneficial in tying the $$p, n$$ together. Otherwise they are predicted independently.
* Discriminator:
  * Discriminates between real and fake $$p, n, s$$
* Misc additional losses:
  * <u>Scene Classification</u> from latent vector: Tries to predict the room category (over 8 scene categories)
  * <u>Pixel level distribution</u>: Predicts pixel-level distribution of different object classes from its semantic prediction (I'm not exactly sure what is being done here though)



## Evaluation

Performed on both SUNCG (synthetic) and Matterport3D (real world) dataset. This is one of the first works in the direction so they created their own simple baselines.